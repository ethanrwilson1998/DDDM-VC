{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# append top level dir to our path\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import utils\n",
    "from data_loader import MelSpectrogramFixed\n",
    "from inference import get_yaapt_f0, load_audio\n",
    "from model.vc_dddm_mixup import DDDM, Wav2vec2\n",
    "from model_f0_vqvae import Quantizer\n",
    "from vocoder.hifigan import HiFi\n",
    "\n",
    "mel_fn, w2v, f0_quantizer, model, net_v = None, None, None, None, None\n",
    "\n",
    "\n",
    "def _get_speaker_embedding(a):\n",
    "    global mel_fn, w2v, f0_quantizer, model, net_v\n",
    "\n",
    "    if mel_fn is None:\n",
    "        mel_fn = MelSpectrogramFixed(\n",
    "            sample_rate=hps.data.sampling_rate,\n",
    "            n_fft=hps.data.filter_length,\n",
    "            win_length=hps.data.win_length,\n",
    "            hop_length=hps.data.hop_length,\n",
    "            f_min=hps.data.mel_fmin,\n",
    "            f_max=hps.data.mel_fmax,\n",
    "            n_mels=hps.data.n_mel_channels,\n",
    "            window_fn=torch.hann_window,\n",
    "        ).cuda()\n",
    "\n",
    "    if w2v is None:\n",
    "        # Load pre-trained w2v (XLS-R)\n",
    "        w2v = Wav2vec2().cuda()\n",
    "\n",
    "    if f0_quantizer is None:\n",
    "        # Load model\n",
    "        f0_quantizer = Quantizer(hps).cuda()\n",
    "        utils.load_checkpoint(a.ckpt_f0_vqvae, f0_quantizer)\n",
    "        f0_quantizer.eval()\n",
    "\n",
    "    if model is None:\n",
    "        model = DDDM(\n",
    "            hps.data.n_mel_channels,\n",
    "            hps.diffusion.spk_dim,\n",
    "            hps.diffusion.dec_dim,\n",
    "            hps.diffusion.beta_min,\n",
    "            hps.diffusion.beta_max,\n",
    "            hps,\n",
    "        ).cuda()\n",
    "        utils.load_checkpoint(a.ckpt_model, model, None)\n",
    "        model.eval()\n",
    "\n",
    "    if net_v is None:\n",
    "        # Load vocoder\n",
    "        net_v = HiFi(\n",
    "            hps.data.n_mel_channels,\n",
    "            hps.train.segment_size // hps.data.hop_length,\n",
    "            **hps.model,\n",
    "        ).cuda()\n",
    "        utils.load_checkpoint(a.ckpt_voc, net_v, None)\n",
    "        net_v.eval().dec.remove_weight_norm()\n",
    "\n",
    "    # Convert audio\n",
    "    # print(\">> Converting each utterance...\")\n",
    "    src_name = os.path.splitext(os.path.basename(a.src_path))[0]\n",
    "    audio = load_audio(a.src_path)\n",
    "\n",
    "    src_mel = mel_fn(audio.cuda())\n",
    "    src_length = torch.LongTensor([src_mel.size(-1)]).cuda()\n",
    "    w2v_x = w2v(F.pad(audio, (40, 40), \"reflect\").cuda())\n",
    "\n",
    "    try:\n",
    "        f0 = get_yaapt_f0(audio.numpy())\n",
    "    except:\n",
    "        f0 = np.zeros((1, audio.shape[-1] // 80), dtype=np.float32)\n",
    "\n",
    "    ii = f0 != 0\n",
    "    f0[ii] = (f0[ii] - f0[ii].mean()) / f0[ii].std()\n",
    "    f0 = torch.FloatTensor(f0).cuda()\n",
    "    f0_code = f0_quantizer.code_extraction(f0)\n",
    "\n",
    "    trg_name = os.path.splitext(os.path.basename(a.trg_path))[0]\n",
    "    trg_audio = load_audio(a.trg_path)\n",
    "\n",
    "    trg_mel = mel_fn(trg_audio.cuda())\n",
    "    trg_length = torch.LongTensor([trg_mel.size(-1)]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        c = model.encode_speaker(\n",
    "            w2v_x,\n",
    "            f0_code,\n",
    "            src_length,\n",
    "            trg_mel,\n",
    "            trg_length,\n",
    "        )\n",
    "        return c.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotmap import DotMap\n",
    "\n",
    "\n",
    "def get_speaker_embedding(path_to_wav):\n",
    "    global hps, device, a\n",
    "    a = DotMap()\n",
    "    a.src_path = path_to_wav\n",
    "    a.trg_path = path_to_wav\n",
    "    a.ckpt_model = \"./ckpt/model_base.pth\"\n",
    "    a.ckpt_voc = \"./vocoder/voc_ckpt.pth\"\n",
    "    a.ckpt_f0_vqvae = \"./f0_vqvae/f0_vqvae.pth\"\n",
    "    a.t = 6\n",
    "    config = os.path.join(os.path.split(a.ckpt_model)[0], \"config.json\")\n",
    "    hps = utils.get_hparams_from_file(config)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return _get_speaker_embedding(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import tqdm\n",
    "\n",
    "for p in tqdm.tqdm(glob.glob(\"D://vox1_test_wav/**/*.wav\", recursive=True)):\n",
    "    folders = p.split(os.sep)\n",
    "    speaker_id = folders[-3]\n",
    "    clip_id = folders[-2]\n",
    "    utterance_id = folders[-1]\n",
    "    emb = get_speaker_embedding(p)\n",
    "    speaker_embeddings.append(\n",
    "        {\n",
    "            \"speaker_id\": speaker_id,\n",
    "            \"clip_id\": clip_id,\n",
    "            \"utterance_id\": utterance_id,\n",
    "            \"embedding\": emb,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(len(speaker_embeddings))\n",
    "df = pd.DataFrame(speaker_embeddings)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"speaker_embeddings.pickle\", \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = 0\n",
    "\n",
    "for i, row1 in tqdm.tqdm(df.iterrows()):\n",
    "    for j, row2 in df.iterrows():\n",
    "        if row1[\"speaker_id\"] == row2[\"speaker_id\"]:\n",
    "            continue\n",
    "\n",
    "        emb1 = np.asarray(row1[\"embedding\"]).squeeze()\n",
    "        emb2 = np.asarray(row2[\"embedding\"]).squeeze()\n",
    "\n",
    "        dist = np.linalg.norm(emb1 - emb2)\n",
    "        if dist > sensitivity:\n",
    "            sensitivity = dist\n",
    "\n",
    "print(f\"sensitivity={sensitivity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"voxceleb1_sensitivity.txt\", \"w\") as file:\n",
    "    file.write(f\"sensitivity={sensitivity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDDM-VC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
